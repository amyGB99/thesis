\chapter{Estado del Arte}\label{chapter:state-of-the-art}

Los sistemas AutoML son capaces de suplir en gran medida muchas de las tareas que antes debía realizar un experto de Machine Learning. Ellos crean sus
propias canalizaciones de ML. Estas canalizaciones suelen incluir preprocesamiento de los datos, selección del mejor modelo y la optimización de hiperparámetros 
(HPO)[\cite{36}].

La diversidad existente en la forma en que se realiza cada una de las fases anteriores hace necesario encontrar estrategias de evaluación para medir 
sus diferencias. Además, para saber según las características de un problema, cuál sistema utilizar. Un benchmark puede verse como una manera de identificar las 
debilidades y fortalezas de una metodología respecto a otra [\cite{2}]. En el ámbito de ML ha sido de gran ayuda como método de evaluación y comparación 
[\cite{5},\cite{6},\cite{46}]. Este hábito ha sido adoptado también por los sistemas de aprendizaje automático. En la formación de estos software de evaluación 
se escogen los conjuntos de datos, las métricas y un método que permitirá medir el rendimiento de una herramienta determinada. La selección del tiempo de ejecución, 
memoria RAM y CPU, también son parte de este proceso si así el programador lo dispone. Estas selecciones comparan los sistemas o métodos ML en predicción. También 
permiten que los resultados obtenidos sean una referencia de cuál herramienta emplear en caso de que el problema inicial tenga limitantes de tiempo y software.

En aras de crear un nuevo benchmark se realizó un estudio de los ya presentes en la literatura para determinar sus características más importantes, 
fallas y resultados más relevantes. De esta manera, evitar cometer los mismos errores e incursionar en sectores menos explorados. Se decidió incluir los más 
populares de ML para ver qué técnicas fueron reintegradas en los de AutoML e identificar las más novedosas que puedan incluirse en la nueva propuesta a crear. Además, 
como trabajos relacionados, una breve reseña sobre los software que evalúan partes de un pipeline de AutoML. El estudio del estado del arte se divide en tres 
subsecciones:
\begin{itemize}
\item Benchmark de ML. 
\item Benchmark de AutoML.
\item Trabajos Relacionados.
\end{itemize}
\section{Benchmark de ML}\label{section:bench_ML}

Los sitios OpenML[\cite{43}] y UCI[\cite{45}] son muy conocidos por permitir la descarga de conjuntos de datos que involucran tareas de ML. A pesar de esto, 
existen muy pocos documentos que recojan la experimentación con los mismos y que ayuden a identificar cuáles modelos de machine learning incluir en una evaluación.

Un paso en la creación de un benchmark es escoger los conjuntos de datos. Existen criterios de selección como el dominio. En ML los dominios más abordados son 
Imágenes [\cite{1},\cite{3}], Texto [\cite{4}], Tabular [\cite{2}] y Series Temporales [\cite{7}], los problemas de grafo son los más escasos [\cite{5}][\cite{6}]. 
Cuando se habla de dominio se hace alusión al significado de cada una de las instancias del conjunto de datos. El formato o estructura en que se encuentran los datos 
es independiente a este. El conjunto puede estar representado como vectores de características numéricas y que sus instancias semánticamente sean oraciones que fueron 
procesadas. Cada uno necesita estar de una forma estandarizada para que los expertos puedan emplearlos sin muchas transformaciones. En la mayoría la forma estandarizada 
encontrada es representado en tabla (filas y columnas) [\cite{4},\cite{1},\cite{3},\cite{2}].

Otro criterio de selección es la tarea que se pretende resolver con cada uno de ellos. Generalmente, solo se enfocan en una tarea, dígase clasificación 
[\cite{4},\cite{1},\cite{3}], regresión [\cite{6}] y clustering [\cite{7}]. La clasificación es la más utilizada en cada una de sus modalidades binaria 
[\cite{4},\cite{2},\cite{5}] y multiclase [\cite{2},\cite{5}]. También en imágenes clasificación multilabel para identificar defectos en las tuberías de 
alcantarillado [\cite{3}]. En el caso de series temporales, el clustering ha demostrado ser de gran utilidad [\cite{7}].

En la recopilación de los dataset el aspecto que más resalta es la búsqueda de diversidad [\cite{4},\cite{2},\cite{6},\cite{7}]. Indican que incluir variedad en 
las meta-características de los conjuntos reduce sesgos de selección[\cite{2}]. Debido a esto se centran en buscar distintos números de columnas, instancias, número 
de clases, entre otras. A pesar de ello, se muestran ejemplos en donde dejan de incluir datos con valores faltantes y con un número de instancias grandes[\cite{2}].

La variedad también puede estar relacionada con el dominio y con las tareas que se resuelven. En grafo [\cite{5},\cite{6}] buscan tareas que abarquen varios 
tipos de predicciones, ya sea a nivel de nodo, enlace o grafo completo. También imponen como característica que los grafos sean semejantes a los de la vida real, 
los cuales sufren de gran escalabilidad. En el dominio texto específicamente detección de falsas noticias [\cite{4}] buscan variedad en el ámbito de aplicación de la 
noticia, ya sea política, económica. El entorno en este tipo de predicciones influye más debido a que los ambientes son engañosos. El tamaño de la 
noticia es otro aspecto importante que intentan diversificar, mientras menor es la longitud de su texto, los sistemas suelen disminuir su rendimiento.

La identificación de la métrica de evaluación es otro paso en la formación del benchmark. Estas se encargan de cuantificar el rendimiento de los modelos. Suelen 
emplear métricas generales para todos los conjuntos [\cite{3},\cite{7},\cite{5},\cite{6}] y luego sacar conclusiones a partir de ellas. La más utilizada es precisión 
[\cite{4},\cite{1},\cite{2}].

Una vez se tienen los conjuntos y la métrica seleccionada, sigue el proceso de evaluación. Para ello, los benchmark de ML recrean un ambiente que hace posible 
descargar de una manera rápida los dataset de manera individual [\cite{2},\cite{3},\cite{5},\cite{6}], y las partes en que se dividen para la evaluación 
[\cite{3},\cite{4},\cite{7}]. Entre las estrategias de división emplean la aleatoriedad (k-fold) [\cite{1},\cite{2}]. Otros [\cite{4},\cite{5},\cite{6}] deciden que 
las características del mismo dominio de aplicación resultan más desafiante para los modelos, ejemplo dividir en partes utilizando una característica de tiempo.

Un ejemplo de utilización de las evaluaciones de los modelos de machine learning es su comparación. Estas comparaciones se realizan de manera incorrecta en muchas 
ocasiones, lo que hace necesario definir los pros y los contras de varias estrategias a seguir [\cite{1}]. Una de ellas es contar el número de veces en las que un 
modelo fue el que obtuvo mejor desempeño, lo que podría conducir a sesgos producto a la distribución de los datos. Otra vía sería comparar por pares, pero ahí se 
asume que todos los algoritmos están diseñados para lograr el mismo resultado. Otra forma es usar el promedio de las métricas de las evaluaciones en el conjunto de 
datos de prueba, es el más utilizado [\cite{3},\cite{2}]. Por último crear un método de evaluación basado en fases diseñado para comparar en igualdad de condiciones 
y teniendo en cuenta las características de cada conjunto de datos [\cite{1},\cite{7}].  

Los benchmark de ML resuelven la limitación de muchas suites de conjuntos de datos que carecen de la presentación de una métrica de optimización para sus conjuntos 
[\cite{44}], como también de un análisis detallado de los metadatos que los conforman. 
Entre las experimentaciones más sobresalientes llevadas a cabo en cada uno de ellos se encuentran la agrupación de las características de los dataset 
en clusters para determinar cuál aporta mejores y peores resultados [\cite{2}]. También la forma en que establecen la relación existente entre una correcta 
extracción de features y el rendimiento del modelo [\cite{4}]. Además, en el dominio grafos los conjuntos de datos propuestos [\cite{5},\cite{6}] presentan un desafío 
a gran escala. Estos han servido como muestra para incentivar a otras investigaciones.

Estos en su mayoría solo incluyen estudios basados en la predicción [\cite{4},\cite{3},\cite{2}]. En diversas ocasiones se desea un modelo que aporte buenos 
resultados, pero también que sea robusto y solo se dispone de cierta disponibilidad de tiempo y memoria lo que habría que investigar aún más es estos temas. 
En la tabla 1 se recogen todas las características de los benchmark de ML abordadas en esta sección. 

\section{Benchmark de AutoML}\label{section:bench_AutoML}

Los marcos de aprendizaje automático en su definición más simple pueden verse como un problema CASH [\cite{37}]. La tarea es seleccionar un modelo y 
una combinación de hiperparámetros que permita predecir resultados sobre un conjunto de datos inicial. Los modelos que conforman estos sistemas pueden 
ser modelos clásicos de ML como SVM, Árboles de Decisión, Regresión Logística entre otros o redes neuronales. Las herramientas que puramente utilizan redes neuronales
resuelven un problema de Búsqueda de Arquitecturas Neuronales (NAS) [\cite{35}] y se les llama sistemas AutoDL. Los restantes que admiten ambos enfoques NAS y CASH se 
les llama AutoML clásico. Ambos forman las dos direcciones de investigación de los sistemas AutoML.

El entrenamiento de las redes neuronales requiere de una alta potencia computacional y gran cantidad de memoria lo que frena los experimentos y pone una barrera de 
entrada a los investigadores sin acceso a computación a gran escala [\cite{49}] [\cite{46}]. Esto podría ser uno de los factores del por qué la mayoría de los 
benchmark están encaminados a sistemas AutoML clásico. En el ámbito de aprendizaje profundo existen benchmark para evaluar modelos NAS. Estos serán vistos en la 
próxima sección debido a que no evalúan el proceso completo de la creación de una canalización de AutoML.

Entre los sistemas AutoDL están AutoKeras [\cite{13}] y AutoPytorch [\cite{21}] y entre los AutoML clásicos: AutoWeka [\cite{8}], AutoSklearn [\cite{9}] y 
AutoGluon  [\cite{17}]. Las evaluaciones de presentación de estas herramientas realizan un estudio sobre las estrategias y técnicas que ponen en práctica cada uno 
de ellos. A pesar de que el objetivo de estas evaluaciones difiere de la creación de un benchmark sus experimentaciones sirven de referencia para otras. Además, 
proveen conjuntos de datos y métricas de evaluación que permiten establecer una comparación con otros sistemas. Debido a esto se incluyen en este estudio.

Los challenges(competiciones) al igual que las evaluaciones de las herramientas de AutoML pueden verse como puntos de referencia. Los conjuntos de datos que 
proponen se han incluido en otros benchmark [\cite{14},\cite{15}]. Su naturaleza competitiva permite que las construcciones 
automáticas creadas por los concursantes peleen para obtener un mejor rendimiento en igualdad de condiciones. Su meta es incentivar a los científicos a crear nuevas 
soluciones automáticas e investigar en nuevas técnicas que mejoren los existentes. Estas competiciones existen tanto para los sistemas AutoML clásico 
[\cite{11},\cite{12}] como para los sistemas AutoDL [\cite{29}].

Para enfatizar en las características principales de los benchmark de AutoML se divide la sección en varios aspectos:

\begin{itemize}
    \item Dominios y Tipos-Estructuras de los Datos de Entrada.
    \item Objetivos Generles.
    \item Estrategias de Selección de los Conjuntos de Datos.
    \item Métodos de Evaluación y Métricas de Optimización. 
    \item Restricciones de Tiempo y  Software.
    \item Resultados.
    \item Fallas. 
    \end{itemize} 

\begin{flushleft} 
    {\large { \textbf{Dominios y Tipos-Estructuras de los Datos de Entrada}}}\label{subsection:dom_AutoML}
\end{flushleft}

Los conjuntos de datos de los benchmark de AutoML al igual que los de ML pertenecen a un dominio. Se agrupan en los que abarcan un solo ámbito texto[\cite{20}], 
imágenes[\cite{23}], tabulares [\cite{14},\cite{25},\cite{26},\cite{30},\cite{32}] y los que incluyen más de uno [\cite{10},\cite{16},\cite{26},\cite{31}]. 
Independientes del dominio sus instancias se encuentran en un formato entendible por los sistemas de aprendizaje automático. El mismo puede estar estructurado o no. 
La forma estructurada son instancias que forman vectores de características y estas pueden ser de tipo numérico, categóricas o tipos no estructurados como texto e 
imágenes. Los conjuntos de datos de los benchmark de AutoML en su mayoría se presentan como características numéricas y/o categóricas 
[\cite{10},\cite{11},\cite{15},\cite{18},\cite{19},\cite{31}]. Algunos[\cite{13},\cite{20},\cite{23}] presentan conjuntos con datos no estructurados. Otros 
[\cite{21},\cite{29}] incluyen ambos.

Al formatear cada instancia como vectores de características se asume que son Independientes lo que muchas veces no se asocia con el significado general de los datos y 
perjudica la semántica de los mismos. En ciertos puntos de evaluación [\cite{23},\cite{29}] el tratar a sus conjuntos como una entidad en vez de características sin 
relación alguna es su prioridad. En otros [\cite{27}], buscan la mejor forma de procesar las entradas de tal manera que los sistemas la entiendan y aprendan del 
dominio al que pertenecen.

Para tener una relación entre las instancias y las salidas a predecir los benchmark generalmente utilizan tablas o vectores. En pocos casos se emplean otras estructuras 
como tensores [\cite{29}].

\begin{flushleft} 
    {\large { \textbf{Objetivos Generales}}}\label{subsection:obj_AutoML}
\end{flushleft}

Los objetivos de los benchmark permiten definir el alcance de su futura utilización. En AutoML todos tienen como propósito general medir el desempeño de las 
herramientas en tareas de ML. Además de efectividad buscan eficiencia [\cite{11},\cite{12},\cite{29}].

El desempeño de un sistema es la medición de su aprendizaje en varios conjuntos de datos. El mismo puede medirse de forma aislada\footnote{Los sistemas cada vez 
que se entrenan aprenden desde cero, no acumulan el conocimiento [53].} o permanente.\footnote{Los sistemas aprenden continuamente. Acumulan el conocimiento, 
aprendiendo del pasado, luego lo adaptan y lo utilizan para ayudar en el aprendizaje futuro [53].}. Existen pocos ejemplos de benchmark que utilicen el aprendizaje 
permanente y el concepto drift\footnote{Este concepto hace alusión al cambio entre las relaciones de los datos de entrada y salida. Puede que las etiquetas con las que 
se estrena el modelo a la hora de predecir ya no sean ciertas o no tan ciertas [\cite{54}] } [\cite{12}].

Las evaluaciones realizadas sobre los sistemas en cada benchmark se utilizan para realizar comparaciones con otros
[\cite{10},\cite{14},\cite{15},\cite{17},\cite{18},\cite{23},\cite{24},\cite{31},\cite{32}]. También para compararlos con expertos haciendo uso de los algoritmos de ML 
[\cite{19},\cite{20},\cite{21}].

En cada una de las propuestas de puntos de evaluación se observa variedad de opiniones respecto a los criterios de selección de los dataset, métricas y demás 
procesos que intervienen en su creación.

\begin{flushleft} 
    {\large { \textbf{Estrategias de Selección de los Conjuntos de Datos}}}\label{subsection:sel_conj_AutoML}
\end{flushleft}

%\subsection{Estrategias de Selección de los Conjuntos de Datos} \label{subsection:sel_conj_AutoML}

Cuando un sistema se evalúa para validar su efectividad, esta evaluación debe tratar de ser un desafío . Si todos ellos ante los conjuntos de datos aportan buenos 
resultados se incumple el objetivo principal al carecer de dificultad [\cite{15}]. De lo anterior se deduce que el criterio de selección de los conjuntos de datos 
presenta fallas en su metodología.

En los benchmark de AutoML las normas de selección de los conjuntos de evaluación varían entre darle más importancia a las tareas que se resuelven 
[\cite{18},\cite{26},\cite{32}] o a ciertas características que presentan cada uno. Otros de los criterios que predomina es el utilizar los dataset con más 
descargas [\cite{19}] o que hayan sido partícipe de otro estudio [\cite{9}]. Estos últimos pueden causar sesgos de selección para las herramientas de nueva generación 
[\cite{15}].

El criterio predominante es sus meta-características [\cite{10},\cite{11},\cite{15},\cite{16},\cite{17},\cite{27},\cite{28}]. Estas propiedades son causantes del 
desajuste de los resultados por parte de los sistemas[\cite{10}].

Entre las características que más resaltan para seleccionar un dataset se encuentran:
\begin{itemize}
    \item Todos los conjuntos deben estar etiquetados, ya que solo se resuelven tareas de aprendizaje supervisado en todos los benchmark estudiados.
    \item Selección de datos reales. Los datos artificiales, los sistemas los solucionan con facilidad [\cite{15}]. Existen ejemplos [\cite{10},\cite{16}] en donde sí 
    se incluyeron datos artificiales. 
    \item Diversidad en sus meta-características [\cite{10},\cite{15},\cite{16},\cite{17},\cite{18},\cite{22},\cite{28}]. Estas propiedades suelen ser
    el número de features, instancias, variables categóricas, entre otras.
    \item Incluir conjuntos con desequilibrio\footnote{El desequilibrio en las clases está dado por la razón entre la clase minoritaria frente a la mayoritaria.} en 
    las clases [\cite{16},\cite{31}]. Cuando estas carecen de información producto al desequilibrio, el rendimiento tiende a decaer [\cite{16}]. Este desbalance puede 
    ser calculado en los datos de entrenamiento [\cite{11},\cite{29}]. En otros casos se calcula en todo el dataset [\cite{31}]. 
    \item Conjuntos con ruido. El ruido [\cite{47}] es una malformación en los datos ya puede ser por presencia de outliers, instancias mal etiquetadas, 
    redundantes. La mayoría de los conjuntos reales por naturaleza están sometidos a algún nivel de ruido producto a errores de recopilación u otros. 
    El aumento de instancias suele provocarlo [\cite{24}]. 
    \item Conjuntos con valores faltantes. A pesar de que muchos sistemas carecen de técnicas de procesamiento de valores faltantes, algunos deciden incluirlos 
    [\cite{10},\cite{11}]. En otros ejemplos [\cite{18}] limitan su empleo, para no requerir una limpieza previa.
    \item Dataset con alta dimensionalidad\footnote{La maldicion de la dimensionalidad puede verse como el bajo número de ejemplos para tantos features.}. 
    Conjuntos con estas características [\cite{11},\cite{24}] complejizan la resolución de la tarea que proponen ya que los sistemas tienen muchos features básicos 
    a seleccionar [\cite{47}].
    \item Dataset multimodales. Los desarrolladores de AutoGluon [\cite{42}] plantean que la mayoría de los trabajos de AutoML se centran en tratar a los datos 
    tabulares (valores numéricos y categóricos), las imágenes y texto por separado. En la vida real estos tipos tienden a coexistir. Entre los benchmark recopilados 
    existe un ejemplo de uso de estos conjuntos [\cite{27}]. 
   
    \end{itemize} 
\begin{flushleft} 
    {\large { \textbf{Métodos de Evaluación y Métricas}}}\label{subsection:met_AutoML}
\end{flushleft}
%\subsection{Métodos de Evaluación y Métricas} \label{subsection:met_AutoML}

Una vez se tienen los conjuntos de datos se escoge el método de evaluación. Es necesario seleccionar una forma para dividir los conjuntos y una métrica de optimización. 
La división interviene en el desempeño de las herramientas de aprendizaje y puede evitar que se ejecuten correctamente.
En ejecuciones donde una clase completa queda en el conjunto de prueba al no tener referencias de esta en el entrenamiento, los marcos pueden fallar [\cite{10}]. 
Las más utilizadas son hold-out y cross-validation (ver tabla 2).

Las métricas para cuantifican el rendimiento en cada uno de los puntos de referencia se seleccionan en dependencia de los datos o del tipo de tarea. Para clasificación 
las más utilizadas son F1[\cite{10}], AUC [\cite{15}] y precisión [\cite{16},\cite{19},\cite{24}] y para regresión el error cuadrático medio 
[\cite{10},\cite{16},\cite{19}]. La precisión desequilibrada también se emplea en aquellos conjuntos con mucho desequilibrio en sus clases [\cite{32}]. Cuando el 
problema posee limitantes de tiempo, requiere utilizar métricas que permitan obtener las predicciones durante cualquier momento en la ejecución[\cite{29}]. La 
estrategia de selección de la métrica más usada por los benchmark [\cite{8},\cite{9},\cite{13},\cite{20},\cite{22},\cite{26}] es seleccionar algunas de manera general 
para todos los conjuntos. Estiman que estas son de utilidad en todos los ejemplos por igual y la más empleada es precisión.

\begin{flushleft} 
    {\large { \textbf{Restricciones de Tiempo y Software}}}\label{subsection:tiempo_AutoML}
\end{flushleft}

Los marcos AutoML deberían ejecutarse por tiempo indefinido hasta que converjan a una solución que ellos consideren óptima, pero esto es muy costoso [\cite{16}]. 
Varios benchmark definen un tiempo de ejecución, ya sea distintivo para cada tarea [\cite{11},\cite{29}], o el mismo para todas (Ver figura 2). La metodología de 
selección que más resalta es probar con varios ejemplos de tiempo y determinar en cuál se obtienen mejores resultados [\cite{15},\cite{31}]. También escoger uno en 
donde un porciento de las tareas hayan logrado completarse [\cite{16}] o en dependencia de la escala de los conjuntos que presentan [\cite{23}]. Algunos no 
especifican el por qué de la selección [\cite{14},\cite{22}]. 

Los software de evaluación de las herramientas de aprendizaje automático son extensibles y de código abierto. Ellos facilitan el proceso de variar el tiempo de 
prueba, reproducir los experimentos y permitir una descarga rápida de los dataset [\cite{10},\cite{15},\cite{23},\cite{28},\cite{31}].
Existen ejemplos[\cite{10},\cite{15},\cite{31}] en donde aportan todo un entorno de máquina para ejecutar las evaluaciones y también permiten ejecuciones de forma 
local. Algunos se limitan a utilizar el software de otros benchmark [\cite{17},\cite{19},\cite{20},\cite{21},\cite{26},\cite{32}]. En el caso de las 
competiciones [\cite{11},\cite{29}] tienen accesibles algunos de sus conjuntos de datos. Ellos mediante su plataforma facilitan la evaluación de soluciones y la 
obtención de los resultados, todos con restricciones de CPU, tiempo y memoria RAM.

Las experimentaciones en casi todos los benchmark se realizan bajo restricciones de RAM y CPU, solo los que tienen un entorno para evaluación pueden hacer que esto 
se cumpla. La mayoría utiliza un baseline como límite del rendimiento mínimo obtenido, el más utilizado RandomForest [\cite{29},\cite{31}].

\begin{flushleft} 
    {\large { \textbf{Resultados Relevantes}}}\label{subsection:result_AutoML}
\end{flushleft}

Estos software de pruebas [\cite{10},\cite{15},\cite{23},\cite{31}] permiten la reproducción de sus experimentaciones aportándolas en su código fuente. Muchas de 
las técnicas que emplean pueden ser reutilizables en otras investigaciones, ya que demuestran ser de gran utilidad. Entre estas se encuentran la visualización de la 
relación entre las meta-características de los conjuntos y el rendimiento de los sistemas mediante gráficos [\cite{10},\cite{26},\cite{30}]. La obtención de 
diferencias entre los resultados de rendimiento mediante pruebas de distribución [\cite{18}] como Shapiro-Wilk, Wilcoxon y Friedman. La selección de diagramas de 
cajas y bigotes [\cite{10},\cite{16},\cite{22},\cite{31}], árboles de Bradley-Terry [\cite{31}] u otros gráficos para visualizar la eficiencia de las herramientas. 
También se evidencian técnicas para establecer la relación entre los resultados y el tiempo, a partir de qué momento tienden a sufrir sobreajuste los sistemas 
[\cite{15}]. Además de verificar si este es suficiente para completar todas las tareas que presentan [\cite{29}].

Las estrategias de comparación son otra de sus contribuciones. La metodología de definición del sistema ganador, al igual que en los benchmark de ML, suele variar. 
La más empleada es el promedio de la métrica de rendimiento [\cite{15}][\cite{31}] y escoger como ganador al que mejores resultados obtuvo en la mayor cantidad de 
conjuntos [\cite{18}] [\cite{10}] [\cite{9}]. También se evidencia la comparación por pares [\cite{10}]. Además, presentan nuevas estrategias siguiendo enfoques 
lexicográficos [\cite{25}] y mediante comparación narrativa[\cite{14}] para dar un sistema vencedor.

Las evaluaciones realizadas [\cite{19},\cite{20},\cite{21}] posibilitan conocer en cuánto supera un sistema AutoML a un experto que utiliza un algoritmo de ML. 
Esto permite cuantificar el acercamiento de los sistemas automáticos al llegar a imitar a los humanos. A pesar de que los humanos superan a las herramientas de 
AutoML estos son más rápidos, lo que hace que los resultados sean notables [\cite{22}]. Respecto a los conjuntos de datos [\cite{30}] dejan en claro las limitaciones 
de alcance que sufren de manera general y más cuando están relacionados con sectores en los que son confidenciales. En conjuntos de fraude [\cite{33}]los datos están 
formados por identidades, estados de cuenta. Los que utilizan datos médicos [\cite{33}] contienen información de seguros y demás. Ambos se vuelven inaccesibles para 
utilizarlos en un estudio.

Además, con las evaluaciones y comparaciones que realizan, los estudios de oblación [\cite{17},\cite{29}] y los análisis cualitativos [\cite{31}] reflejan las 
deficiencias y ventajas de utilizar ciertos sistemas. Resaltando su incapacidad de utilizar datos no etiquetados [\cite{26}] y bajo rendimientos en conjuntos 
desequilibrados [\cite{30},\cite{32}]. También están restringidos respecto a los formatos de los datos de entrada que reciben [\cite{27}]. Una de las más importantes 
es la inferioridad de estos ante los humanos debido a su baja capacidad de reconocimiento de características de los dominios [\cite{19}].

\begin{flushleft} 
    {\large { \textbf{Fallas}}}\label{subsection:fallas_AutoML}
\end{flushleft}
Las fallas que se presentan en cada software de prueba son igual de importantes que los resultados que se derivan de ellos, permiten aprender de los errores. 
Estas pueden dividirse en fallas en las ejecuciones de los sistemas, de los experimentos realizados y las fallas en la metodología de su creación.

Los problemas en las ejecuciones de los sistemas AutoML que más se repiten en cada evaluación es la incapacidad de tratar con valores faltantes 
[\cite{10},\cite{22},\cite{32}] y con algunos tipos de variables [\cite{30}]. También por conjuntos con un tamaño muy grande que no logran completarse 
[\cite{12},\cite{22}]. Las que más predominan son las fallas en la memoria [\cite{10},\cite{11},\cite{12},\cite{22},\cite{29}] y ejecuciones no terminadas producto al 
tiempo [\cite{10},\cite{12},\cite{19},\cite{20},\cite{29}].

Los errores en los experimentos son afectaciones que sufren las comparaciones y resultados producto de las fallas de los sistemas y malas decisiones para abordarlas. 
Entre estas resaltan la ejecución de las pruebas con desigualdad de recursos para cada herramienta [\cite{10},\cite{25}] rompiendo con la equidad de las evaluaciones.

Respecto a la metodología de los sistemas de prueba se presentan algunos inconvenientes como la incapacidad de descargar los dataset que utilizan [\cite{14},\cite{32}] y 
sus divisiones para la evaluación [\cite{10}, \cite{14},\cite{16},\cite{34}]. Los challenges [\cite{11},\cite{12},\cite{29}] ocultan muchos de sus conjuntos de datos 
al público. A pesar de esto, a través de la plataforma sí permiten la obtención de resultados, aunque estos son difíciles de interpretar y de que sobrepasen la línea 
base, debido a su carácter competitivo [\cite{31}]. También existen limitaciones relacionadas a las características de los dataset, son demasiado pequeños 
[\cite{15},\cite{23}], sin valores faltantes [\cite{18},\cite{24},\cite{25}] y el dominio de los mismos es irrelevante en su selección 
[\cite{10},\cite{16},\cite{18},\cite{22},\cite{31}]. Además, se encuentran muy procesados\footnote{Limpieza en los conjuntos puede incluir imputación de valores 
faltantes, codificación de variables categóricas, o de string.} que sufren de falta de similitud con los datos reales [\cite{18},\cite{22}]. Los recursos de software y 
tiempo para las evaluaciones en algunos ejemplos se escogieron arbitrariamente [\cite{8}] o no se especifica el porqué de la selección [\cite{13},\cite{22},\cite{24}].

\section{Trabajos Relacionados}\label{section:trabajos_relacionados}

\begin{table}
    \centering
    \resizebox{15cm}{!} {
    \begin{tabular}{|l|c|c|c|c|c|c|}
    \hline
    Bench&Tabular&Sustituto&Tarea& \# Conjunt &Varias Tareas&Consultante\\ \hline
    NAS-Bench101  & \checkmark  &  & Image & 1  &  &  \checkmark  \\
    NAS-Bench1-Shot & \checkmark & &Image  & 1 & & \checkmark   \\
    NAS-Bench-201  & \checkmark  &   & Image & 3 & &  \checkmark   \\ 
    NAS-Bench-301  & & \checkmark &  Image & 1  & &   \checkmark   \\
    NAS-Bench-111  & & \checkmark &  Image & 1  & &   \checkmark   \\
    NAS-Bench-311  & & \checkmark &  Image & 1  & &   \checkmark   \\
    NASBenchNLP11  & & \checkmark &  NLP & 1 & &  \checkmark     \\
    Suite  & \checkmark  & \checkmark & Image/NLP/VC & 25 & \checkmark &      \checkmark         \\
    NAS-Bench-360  & &  & NAS & 10 & \checkmark &  \\ \hline
    \end{tabular}
    \caption{benchmark NAS}
    \label{fig:NAS}
    }
\end{table}

En lugar de comparar marcos completos de aprendizaje automático, a menudo es útil centrarse en varias subpartes y optimizarlas paso a paso.

Entre estas partes se encuentra la optimización de hiperparámetros la cual es un problema central en la línea de aprendizaje automático. Esta aún presenta 
inconvenientes en la evaluación de sus modelos debido a las limitaciones computacionales para abordar el tamaño de los espacios de búsqueda de cada uno de ellos. Estos 
algoritmos están divididos en dos grandes ramas: Caja Negra[\cite{35}] y Multi-Fidelidad[\cite{35}]. El estudio de estas dos categorías no se ha hecho esperar. 
Los investigadores proponen crear benchmark que permitan resolver un problema general, la verificación de su rendimiento. Esta necesidad surge cuando los resultados de 
un método que funciona bien en el protocolo experimental de una publicación no se pueden replicar; o cuando el método tiene un rendimiento inferior en un protocolo 
empírico ligeramente diferente [\cite{61}]. 

Entre los métodos de Caja Negra se encuentran los de optimización bayesiana que son ampliamente utilizados en las implementaciones de sistemas AutoML. Estos modelos
aportan mejores resultados que Búsqueda en Cuadrícula y Aleatoria y superan en muchos casos a los humanos. HPOlib[\cite{63}] es una de las primeras 
bibliotecas que recoge puntos de referencias de la literatura y evalúa métodos bayesianos. Esta está formada por 3 benchmark que abarcan dimensionalidad alta, baja y 
media de hiperparámetros a optimizar. Utiliza datos reales y pruebas sintéticas. AClib[\cite{62}] está encaminado a algoritmos de Búsqueda Local Estocástica (SLS) 
de manera general. Este emplea contenedores para cada espacio de configuración de hiperparámetro, le permite garantizar que los límites de memoria y tiempo que imponen 
se cumplen. 

HPO-B[\cite{61}] y COCO[\cite{60}] también están dirigidos a métodos de Caja Negra. HPO-B es uno de los más grande hasta el momento y sus conjuntos de datos son 
tomados de OpenML. Estos se encontran en un inicio sin procesar y luego son sometidos a limpieza, pre-procesamiento y organización. 
Su objetivo específico es evaluar métodos por transferencia mediante un protocolo empírico. COCO por otro lado solo compara los de dimensión cero en donde los datos 
provienen de naturaleza estocástica y deterministas y se evalúan en problemas mono y multiobjetos.

Un sucesor de HPO-B en escalabilidad y en proveer varios puntos de referencia es HPO-Bench[\cite{50}]. Este tiene su enfoque en problemas de multifidelidad y 
dispone sus conjuntos tanto en formato tabular como no estructurado. 

NAS[\cite{35}] es otra de las partes que pueden formar una canalización de AutoML. Estos métodos tienen una fuerte atención debido a su efectividad en el diseño de 
redes neuronales de última generación. Sin embargo, al igual que HPO[\cite{35}] producto a su gran complejidad computacional hacen extremadamente difícil sus 
comparaciones y la reproducción de investigaciones [\cite{55}][\cite{59}].

Una forma de abordar limitaciones de costo y tiempo es construir benchmark de NAS tabulares. Estos permiten tener las evaluaciones 
pre calculadas para todas las arquitecturas posibles del espacio de búsqueda que describen. NAS-Bench-101[\cite{49}] es uno de los primeros en surgir con estas 
características. Este posibilita la evaluación de muchos métodos NAS en un corto período de tiempo, pero aún tiene ciertas limitantes. Entre estas la incapacidad de 
evaluación y comparación de métodos one-shot. NAS-Bench-1-shot[\cite{55}] mediante una adaptación del espacio de búsqueda de NAS-Bench-101 resolve este inconveniente. 
NAS-Bench-201[\cite{56}] persigue el mismo propósito que NAS-Bench-Shot-101 pero crea un punto de referencia totalmente nuevo en vez de adaptar el existente. 
NAS-Bench-201 por otro lado pretende ser independiente del modelo de evaluación buscando aún más generalidad aumentando el número de celdas entrenadas de su antecesor 
NAS-Bench-101.

Estos benchmark cumplen con su objetivo de verificar el rendimiento de los modelos de NAS. Sin embargo, al representarse tubularmente se basan en una 
evaluación exhaustiva de todas las arquitecturas en un espacio de búsqueda poco realista. Surrugate-NAS[\cite{66}] presenta un nuevo enfoque para puntos de 
referencia de NAS denominado surrugate (sustituto). Estos proveen un método sustituto el cual pueden utilizar para predecir el rendimiento de cualquier arquitectura 
en el espacio de búsqueda. Surrugate-NAS define una metodología general para la creación de estos tipos de benchamrk. Al igual que los tabulares estos permiten la 
misma interfaz de consultar el rendimiento económicamente. En esta categoría se encuentran NAS-Bench-301[\cite{57}] ,NAS-Bench-111[\cite{58}], 
NAS-Bench-311[\cite{58}], NAS-Bench-NLP111[\cite{58}].

Todos estos benchmark se centran en crear espacios de búsqueda más grandes y sofisticados, pero están restringidos a tareas específicas como clasificación de imágenes 
y procesamiento del lenguaje natural. NAS-Bench-360[\cite{51}] por otro lado deja de buscar el mejor espacio de búsqueda para ampliar el dominio de aplicación, el 
tamaño de los conjuntos de datos, dimensionalidad del problema y los objetivos de aprendizaje.

NAS-Bench-Suite[\cite{58}] es un conjunto de benchmark para NAS. Este agrupa casi todos los software de evaluación anteriores. Esto permite probar tareas tanto de 
imágenes como procesamiento del lenguaje natural, y reconocimiento de voz. Al contrario de NAS-Bench-360 que la evaluación puede tardar horas de CPU, esta suite al 
utilizar benchmark consultables tarda 5 minutos en obtener resultados.

En la tabla \ref{fig:NAS} puede ver alguna de las características de los NAS expresadas en esta sección.




%\begin{table}[]
 %   \caption{benchmark NAS}
  %  \vspace{20pt}
   % \centering
    %\begin{tabular}{|c|c|c|c|c|c|}

    %\begin{tabular}{p{2.2cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}
     %   \hline
       % Feature & NAS-Bench101 & NAS-Bench201 & NAS-Bench-301 & NAS-Bench-111 & NAS-Bench-311 & NASBenchNLP11 & Suite & NAS Bench 
        %\multicolumn{1,4}{ |c| }{Feature} \\ \hline
      %  Bench  & Tabular  & Sustituto & Tarea & Cantidad (Tareas) & tipos de tareas & Benchmark consultante  \\
      
       % NAS-Bench101  & \checkmark  & \checkmark &  &  & &     \\ \hline
       
        %NAS-Bench1-Shot  & \checkmark & \checkmark &  &  & &  \\ \hline
       
        %NAS-Bench201  & \checkmark  & \checkmark &  &  & & \\ \hline
       % NAS-Bench-301  & \checkmark  & \checkmark &  &  & & \\ \hline
       % NAS-Bench-111  & \checkmark  & \checkmark &  &  & &  \\ \hline
       % NAS-Bench-311  & \checkmark  & \checkmark &  &  & & \\ \hline
        %NASBenchNLP11  & \checkmark  & \checkmark &  &  & &  \\ \hline
        %Suite  & \checkmark  & \checkmark &  &  & &  \\ \hline
        %NAS-Bench-360  & \checkmark  & \checkmark &  &  & & \\
        %\hline
    %\end{tabular}
    %\label{bs2}
%{table}

